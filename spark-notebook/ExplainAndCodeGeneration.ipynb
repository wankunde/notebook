{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark help function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "benchmark: (name: String)(f: => Unit)Unit\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def benchmark(name: String)(f: => Unit) {\n",
    "  val startTime = System.nanoTime\n",
    "  f\n",
    "  val endTime = System.nanoTime\n",
    "  println(s\"Time taken in $name: \" + (endTime - startTime).toDouble / 1000000000 + \" seconds\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and set configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: String = true\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.codegen.wholeStage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.codegen.wholeStage\", true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|           sum(id)|\n",
      "+------------------+\n",
      "|499999999500000000|\n",
      "+------------------+\n",
      "\n",
      "Time taken in Spark 2.0: 5.384864876 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(\"Spark 2.0\") {\n",
    "  spark.range(1000L * 1000 * 1000).selectExpr(\"sum(id)\").show()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('count(1), Some(org.apache.spark.sql.Column$$Lambda$2986/0x0000000841215040@2d4cb313))]\n",
      "+- Project [id#33L]\n",
      "   +- Join Inner, (id#33L = id#35L)\n",
      "      :- Range (0, 1000000000, step=1, splits=Some(4))\n",
      "      +- Range (0, 1000, step=1, splits=Some(4))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "count(1): bigint\n",
      "Aggregate [count(1) AS count(1)#40L]\n",
      "+- Project [id#33L]\n",
      "   +- Join Inner, (id#33L = id#35L)\n",
      "      :- Range (0, 1000000000, step=1, splits=Some(4))\n",
      "      +- Range (0, 1000, step=1, splits=Some(4))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [count(1) AS count(1)#40L]\n",
      "+- Project\n",
      "   +- Join Inner, (id#33L = id#35L)\n",
      "      :- Range (0, 1000000000, step=1, splits=Some(4))\n",
      "      +- Range (0, 1000, step=1, splits=Some(4))\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) HashAggregate(keys=[], functions=[count(1)], output=[count(1)#40L])\n",
      "+- Exchange SinglePartition, true, [id=#94]\n",
      "   +- *(2) HashAggregate(keys=[], functions=[partial_count(1)], output=[count#43L])\n",
      "      +- *(2) Project\n",
      "         +- *(2) BroadcastHashJoin [id#33L], [id#35L], Inner, BuildRight\n",
      "            :- *(2) Range (0, 1000000000, step=1, splits=4)\n",
      "            +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false])), [id=#88]\n",
      "               +- *(1) Range (0, 1000, step=1, splits=4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1000L * 1000 * 1000).join(spark.range(1000L).toDF(), \"id\").selectExpr(\"count(*)\").explain(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Debug tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.execution.debug._\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.execution.debug._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q: org.apache.spark.sql.DataFrame = [sum(id): bigint]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val q = spark.range(1,10).toDF().filter('id>3).selectExpr(\"sum(id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.NullPointerException",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "java.lang.NullPointerException",
      "  at org.apache.spark.sql.execution.SparkPlan.sparkContext(SparkPlan.scala:67)",
      "  at org.apache.spark.sql.execution.debug.package$DebugExec.<init>(package.scala:225)",
      "  at org.apache.spark.sql.execution.debug.package$DebugQuery$$anonfun$1.applyOrElse(package.scala:174)",
      "  at org.apache.spark.sql.execution.debug.package$DebugQuery$$anonfun$1.applyOrElse(package.scala:171)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:309)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:309)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:298)",
      "  at org.apache.spark.sql.execution.debug.package$DebugQuery.debug(package.scala:171)",
      "  ... 40 elided",
      ""
     ]
    }
   ],
   "source": [
    "q.debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 WholeStageCodegen subtrees.\n",
      "== Subtree 1 / 2 (maxMethodCodeSize:282; maxConstantPoolSize:208(0.32% used); numInnerClasses:0) ==\n",
      "*(1) HashAggregate(keys=[], functions=[partial_sum(id#44L)], output=[sum#51L])\n",
      "+- *(1) Filter (id#44L > 3)\n",
      "   +- *(1) Range (1, 10, step=1, splits=4)\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean agg_initAgg_0;\n",
      "/* 010 */   private boolean agg_bufIsNull_0;\n",
      "/* 011 */   private long agg_bufValue_0;\n",
      "/* 012 */   private boolean range_initRange_0;\n",
      "/* 013 */   private long range_nextIndex_0;\n",
      "/* 014 */   private TaskContext range_taskContext_0;\n",
      "/* 015 */   private InputMetrics range_inputMetrics_0;\n",
      "/* 016 */   private long range_batchEnd_0;\n",
      "/* 017 */   private long range_numElementsTodo_0;\n",
      "/* 018 */   private boolean agg_agg_isNull_3_0;\n",
      "/* 019 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] range_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[4];\n",
      "/* 020 */\n",
      "/* 021 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 022 */     this.references = references;\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 026 */     partitionIndex = index;\n",
      "/* 027 */     this.inputs = inputs;\n",
      "/* 028 */\n",
      "/* 029 */     range_taskContext_0 = TaskContext.get();\n",
      "/* 030 */     range_inputMetrics_0 = range_taskContext_0.taskMetrics().inputMetrics();\n",
      "/* 031 */     range_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 032 */     range_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 033 */     range_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 034 */     range_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 035 */\n",
      "/* 036 */   }\n",
      "/* 037 */\n",
      "/* 038 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {\n",
      "/* 039 */     // initialize aggregation buffer\n",
      "/* 040 */     agg_bufIsNull_0 = true;\n",
      "/* 041 */     agg_bufValue_0 = -1L;\n",
      "/* 042 */\n",
      "/* 043 */     // initialize Range\n",
      "/* 044 */     if (!range_initRange_0) {\n",
      "/* 045 */       range_initRange_0 = true;\n",
      "/* 046 */       initRange(partitionIndex);\n",
      "/* 047 */     }\n",
      "/* 048 */\n",
      "/* 049 */     while (true) {\n",
      "/* 050 */       if (range_nextIndex_0 == range_batchEnd_0) {\n",
      "/* 051 */         long range_nextBatchTodo_0;\n",
      "/* 052 */         if (range_numElementsTodo_0 > 1000L) {\n",
      "/* 053 */           range_nextBatchTodo_0 = 1000L;\n",
      "/* 054 */           range_numElementsTodo_0 -= 1000L;\n",
      "/* 055 */         } else {\n",
      "/* 056 */           range_nextBatchTodo_0 = range_numElementsTodo_0;\n",
      "/* 057 */           range_numElementsTodo_0 = 0;\n",
      "/* 058 */           if (range_nextBatchTodo_0 == 0) break;\n",
      "/* 059 */         }\n",
      "/* 060 */         range_batchEnd_0 += range_nextBatchTodo_0 * 1L;\n",
      "/* 061 */       }\n",
      "/* 062 */\n",
      "/* 063 */       int range_localEnd_0 = (int)((range_batchEnd_0 - range_nextIndex_0) / 1L);\n",
      "/* 064 */       for (int range_localIdx_0 = 0; range_localIdx_0 < range_localEnd_0; range_localIdx_0++) {\n",
      "/* 065 */         long range_value_0 = ((long)range_localIdx_0 * 1L) + range_nextIndex_0;\n",
      "/* 066 */\n",
      "/* 067 */         do {\n",
      "/* 068 */           boolean filter_value_0 = false;\n",
      "/* 069 */           filter_value_0 = range_value_0 > 3L;\n",
      "/* 070 */           if (!filter_value_0) continue;\n",
      "/* 071 */\n",
      "/* 072 */           ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);\n",
      "/* 073 */\n",
      "/* 074 */           agg_doConsume_0(range_value_0);\n",
      "/* 075 */\n",
      "/* 076 */         } while(false);\n",
      "/* 077 */\n",
      "/* 078 */         // shouldStop check is eliminated\n",
      "/* 079 */       }\n",
      "/* 080 */       range_nextIndex_0 = range_batchEnd_0;\n",
      "/* 081 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(range_localEnd_0);\n",
      "/* 082 */       range_inputMetrics_0.incRecordsRead(range_localEnd_0);\n",
      "/* 083 */       range_taskContext_0.killTaskIfInterrupted();\n",
      "/* 084 */     }\n",
      "/* 085 */\n",
      "/* 086 */   }\n",
      "/* 087 */\n",
      "/* 088 */   private void initRange(int idx) {\n",
      "/* 089 */     java.math.BigInteger index = java.math.BigInteger.valueOf(idx);\n",
      "/* 090 */     java.math.BigInteger numSlice = java.math.BigInteger.valueOf(4L);\n",
      "/* 091 */     java.math.BigInteger numElement = java.math.BigInteger.valueOf(9L);\n",
      "/* 092 */     java.math.BigInteger step = java.math.BigInteger.valueOf(1L);\n",
      "/* 093 */     java.math.BigInteger start = java.math.BigInteger.valueOf(1L);\n",
      "/* 094 */     long partitionEnd;\n",
      "/* 095 */\n",
      "/* 096 */     java.math.BigInteger st = index.multiply(numElement).divide(numSlice).multiply(step).add(start);\n",
      "/* 097 */     if (st.compareTo(java.math.BigInteger.valueOf(Long.MAX_VALUE)) > 0) {\n",
      "/* 098 */       range_nextIndex_0 = Long.MAX_VALUE;\n",
      "/* 099 */     } else if (st.compareTo(java.math.BigInteger.valueOf(Long.MIN_VALUE)) < 0) {\n",
      "/* 100 */       range_nextIndex_0 = Long.MIN_VALUE;\n",
      "/* 101 */     } else {\n",
      "/* 102 */       range_nextIndex_0 = st.longValue();\n",
      "/* 103 */     }\n",
      "/* 104 */     range_batchEnd_0 = range_nextIndex_0;\n",
      "/* 105 */\n",
      "/* 106 */     java.math.BigInteger end = index.add(java.math.BigInteger.ONE).multiply(numElement).divide(numSlice)\n",
      "/* 107 */     .multiply(step).add(start);\n",
      "/* 108 */     if (end.compareTo(java.math.BigInteger.valueOf(Long.MAX_VALUE)) > 0) {\n",
      "/* 109 */       partitionEnd = Long.MAX_VALUE;\n",
      "/* 110 */     } else if (end.compareTo(java.math.BigInteger.valueOf(Long.MIN_VALUE)) < 0) {\n",
      "/* 111 */       partitionEnd = Long.MIN_VALUE;\n",
      "/* 112 */     } else {\n",
      "/* 113 */       partitionEnd = end.longValue();\n",
      "/* 114 */     }\n",
      "/* 115 */\n",
      "/* 116 */     java.math.BigInteger startToEnd = java.math.BigInteger.valueOf(partitionEnd).subtract(\n",
      "/* 117 */       java.math.BigInteger.valueOf(range_nextIndex_0));\n",
      "/* 118 */     range_numElementsTodo_0  = startToEnd.divide(step).longValue();\n",
      "/* 119 */     if (range_numElementsTodo_0 < 0) {\n",
      "/* 120 */       range_numElementsTodo_0 = 0;\n",
      "/* 121 */     } else if (startToEnd.remainder(step).compareTo(java.math.BigInteger.valueOf(0L)) != 0) {\n",
      "/* 122 */       range_numElementsTodo_0++;\n",
      "/* 123 */     }\n",
      "/* 124 */   }\n",
      "/* 125 */\n",
      "/* 126 */   private void agg_doConsume_0(long agg_expr_0_0) throws java.io.IOException {\n",
      "/* 127 */     // do aggregate\n",
      "/* 128 */     // common sub-expressions\n",
      "/* 129 */\n",
      "/* 130 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 131 */\n",
      "/* 132 */     agg_agg_isNull_3_0 = true;\n",
      "/* 133 */     long agg_value_3 = -1L;\n",
      "/* 134 */     do {\n",
      "/* 135 */       if (!agg_bufIsNull_0) {\n",
      "/* 136 */         agg_agg_isNull_3_0 = false;\n",
      "/* 137 */         agg_value_3 = agg_bufValue_0;\n",
      "/* 138 */         continue;\n",
      "/* 139 */       }\n",
      "/* 140 */\n",
      "/* 141 */       if (!false) {\n",
      "/* 142 */         agg_agg_isNull_3_0 = false;\n",
      "/* 143 */         agg_value_3 = 0L;\n",
      "/* 144 */         continue;\n",
      "/* 145 */       }\n",
      "/* 146 */\n",
      "/* 147 */     } while (false);\n",
      "/* 148 */\n",
      "/* 149 */     long agg_value_2 = -1L;\n",
      "/* 150 */\n",
      "/* 151 */     agg_value_2 = agg_value_3 + agg_expr_0_0;\n",
      "/* 152 */\n",
      "/* 153 */     agg_bufIsNull_0 = false;\n",
      "/* 154 */     agg_bufValue_0 = agg_value_2;\n",
      "/* 155 */\n",
      "/* 156 */   }\n",
      "/* 157 */\n",
      "/* 158 */   protected void processNext() throws java.io.IOException {\n",
      "/* 159 */     while (!agg_initAgg_0) {\n",
      "/* 160 */       agg_initAgg_0 = true;\n",
      "/* 161 */       long agg_beforeAgg_0 = System.nanoTime();\n",
      "/* 162 */       agg_doAggregateWithoutKey_0();\n",
      "/* 163 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);\n",
      "/* 164 */\n",
      "/* 165 */       // output the result\n",
      "/* 166 */\n",
      "/* 167 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* numOutputRows */).add(1);\n",
      "/* 168 */       range_mutableStateArray_0[3].reset();\n",
      "/* 169 */\n",
      "/* 170 */       range_mutableStateArray_0[3].zeroOutNullBytes();\n",
      "/* 171 */\n",
      "/* 172 */       if (agg_bufIsNull_0) {\n",
      "/* 173 */         range_mutableStateArray_0[3].setNullAt(0);\n",
      "/* 174 */       } else {\n",
      "/* 175 */         range_mutableStateArray_0[3].write(0, agg_bufValue_0);\n",
      "/* 176 */       }\n",
      "/* 177 */       append((range_mutableStateArray_0[3].getRow()));\n",
      "/* 178 */     }\n",
      "/* 179 */   }\n",
      "/* 180 */\n",
      "/* 181 */ }\n",
      "\n",
      "== Subtree 2 / 2 (maxMethodCodeSize:139; maxConstantPoolSize:137(0.21% used); numInnerClasses:0) ==\n",
      "*(2) HashAggregate(keys=[], functions=[sum(id#44L)], output=[sum(id)#48L])\n",
      "+- Exchange SinglePartition, true, [id=#115]\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_sum(id#44L)], output=[sum#51L])\n",
      "      +- *(1) Filter (id#44L > 3)\n",
      "         +- *(1) Range (1, 10, step=1, splits=4)\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=2\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean agg_initAgg_0;\n",
      "/* 010 */   private boolean agg_bufIsNull_0;\n",
      "/* 011 */   private long agg_bufValue_0;\n",
      "/* 012 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 013 */   private boolean agg_agg_isNull_3_0;\n",
      "/* 014 */   private boolean agg_agg_isNull_5_0;\n",
      "/* 015 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 016 */\n",
      "/* 017 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n",
      "/* 018 */     this.references = references;\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 022 */     partitionIndex = index;\n",
      "/* 023 */     this.inputs = inputs;\n",
      "/* 024 */\n",
      "/* 025 */     inputadapter_input_0 = inputs[0];\n",
      "/* 026 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 027 */\n",
      "/* 028 */   }\n",
      "/* 029 */\n",
      "/* 030 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {\n",
      "/* 031 */     // initialize aggregation buffer\n",
      "/* 032 */     agg_bufIsNull_0 = true;\n",
      "/* 033 */     agg_bufValue_0 = -1L;\n",
      "/* 034 */\n",
      "/* 035 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 036 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 037 */\n",
      "/* 038 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 039 */       long inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 040 */       -1L : (inputadapter_row_0.getLong(0));\n",
      "/* 041 */\n",
      "/* 042 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0);\n",
      "/* 043 */       // shouldStop check is eliminated\n",
      "/* 044 */     }\n",
      "/* 045 */\n",
      "/* 046 */   }\n",
      "/* 047 */\n",
      "/* 048 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, long agg_expr_0_0, boolean agg_exprIsNull_0_0) throws java.io.IOException {\n",
      "/* 049 */     // do aggregate\n",
      "/* 050 */     // common sub-expressions\n",
      "/* 051 */\n",
      "/* 052 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 053 */\n",
      "/* 054 */     agg_agg_isNull_3_0 = true;\n",
      "/* 055 */     long agg_value_3 = -1L;\n",
      "/* 056 */     do {\n",
      "/* 057 */       boolean agg_isNull_4 = true;\n",
      "/* 058 */       long agg_value_4 = -1L;\n",
      "/* 059 */       agg_agg_isNull_5_0 = true;\n",
      "/* 060 */       long agg_value_5 = -1L;\n",
      "/* 061 */       do {\n",
      "/* 062 */         if (!agg_bufIsNull_0) {\n",
      "/* 063 */           agg_agg_isNull_5_0 = false;\n",
      "/* 064 */           agg_value_5 = agg_bufValue_0;\n",
      "/* 065 */           continue;\n",
      "/* 066 */         }\n",
      "/* 067 */\n",
      "/* 068 */         if (!false) {\n",
      "/* 069 */           agg_agg_isNull_5_0 = false;\n",
      "/* 070 */           agg_value_5 = 0L;\n",
      "/* 071 */           continue;\n",
      "/* 072 */         }\n",
      "/* 073 */\n",
      "/* 074 */       } while (false);\n",
      "/* 075 */\n",
      "/* 076 */       if (!agg_exprIsNull_0_0) {\n",
      "/* 077 */         agg_isNull_4 = false; // resultCode could change nullability.\n",
      "/* 078 */\n",
      "/* 079 */         agg_value_4 = agg_value_5 + agg_expr_0_0;\n",
      "/* 080 */\n",
      "/* 081 */       }\n",
      "/* 082 */       if (!agg_isNull_4) {\n",
      "/* 083 */         agg_agg_isNull_3_0 = false;\n",
      "/* 084 */         agg_value_3 = agg_value_4;\n",
      "/* 085 */         continue;\n",
      "/* 086 */       }\n",
      "/* 087 */\n",
      "/* 088 */       if (!agg_bufIsNull_0) {\n",
      "/* 089 */         agg_agg_isNull_3_0 = false;\n",
      "/* 090 */         agg_value_3 = agg_bufValue_0;\n",
      "/* 091 */         continue;\n",
      "/* 092 */       }\n",
      "/* 093 */\n",
      "/* 094 */     } while (false);\n",
      "/* 095 */\n",
      "/* 096 */     agg_bufIsNull_0 = agg_agg_isNull_3_0;\n",
      "/* 097 */     agg_bufValue_0 = agg_value_3;\n",
      "/* 098 */\n",
      "/* 099 */   }\n",
      "/* 100 */\n",
      "/* 101 */   protected void processNext() throws java.io.IOException {\n",
      "/* 102 */     while (!agg_initAgg_0) {\n",
      "/* 103 */       agg_initAgg_0 = true;\n",
      "/* 104 */       long agg_beforeAgg_0 = System.nanoTime();\n",
      "/* 105 */       agg_doAggregateWithoutKey_0();\n",
      "/* 106 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);\n",
      "/* 107 */\n",
      "/* 108 */       // output the result\n",
      "/* 109 */\n",
      "/* 110 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 111 */       agg_mutableStateArray_0[0].reset();\n",
      "/* 112 */\n",
      "/* 113 */       agg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 114 */\n",
      "/* 115 */       if (agg_bufIsNull_0) {\n",
      "/* 116 */         agg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 117 */       } else {\n",
      "/* 118 */         agg_mutableStateArray_0[0].write(0, agg_bufValue_0);\n",
      "/* 119 */       }\n",
      "/* 120 */       append((agg_mutableStateArray_0[0].getRow()));\n",
      "/* 121 */     }\n",
      "/* 122 */   }\n",
      "/* 123 */\n",
      "/* 124 */ }\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q.debugCodegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 WholeStageCodegen subtrees.\n",
      "== Subtree 1 / 2 (maxMethodCodeSize:282; maxConstantPoolSize:208(0.32% used); numInnerClasses:0) ==\n",
      "*(1) HashAggregate(keys=[], functions=[partial_sum(id#52L)], output=[sum#59L])\n",
      "+- *(1) Filter (id#52L > 3)\n",
      "   +- *(1) Range (1, 10, step=1, splits=4)\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean agg_initAgg_0;\n",
      "/* 010 */   private boolean agg_bufIsNull_0;\n",
      "/* 011 */   private long agg_bufValue_0;\n",
      "/* 012 */   private boolean range_initRange_0;\n",
      "/* 013 */   private long range_nextIndex_0;\n",
      "/* 014 */   private TaskContext range_taskContext_0;\n",
      "/* 015 */   private InputMetrics range_inputMetrics_0;\n",
      "/* 016 */   private long range_batchEnd_0;\n",
      "/* 017 */   private long range_numElementsTodo_0;\n",
      "/* 018 */   private boolean agg_agg_isNull_3_0;\n",
      "/* 019 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] range_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[4];\n",
      "/* 020 */\n",
      "/* 021 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 022 */     this.references = references;\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 026 */     partitionIndex = index;\n",
      "/* 027 */     this.inputs = inputs;\n",
      "/* 028 */\n",
      "/* 029 */     range_taskContext_0 = TaskContext.get();\n",
      "/* 030 */     range_inputMetrics_0 = range_taskContext_0.taskMetrics().inputMetrics();\n",
      "/* 031 */     range_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 032 */     range_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 033 */     range_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 034 */     range_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 035 */\n",
      "/* 036 */   }\n",
      "/* 037 */\n",
      "/* 038 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {\n",
      "/* 039 */     // initialize aggregation buffer\n",
      "/* 040 */     agg_bufIsNull_0 = true;\n",
      "/* 041 */     agg_bufValue_0 = -1L;\n",
      "/* 042 */\n",
      "/* 043 */     // initialize Range\n",
      "/* 044 */     if (!range_initRange_0) {\n",
      "/* 045 */       range_initRange_0 = true;\n",
      "/* 046 */       initRange(partitionIndex);\n",
      "/* 047 */     }\n",
      "/* 048 */\n",
      "/* 049 */     while (true) {\n",
      "/* 050 */       if (range_nextIndex_0 == range_batchEnd_0) {\n",
      "/* 051 */         long range_nextBatchTodo_0;\n",
      "/* 052 */         if (range_numElementsTodo_0 > 1000L) {\n",
      "/* 053 */           range_nextBatchTodo_0 = 1000L;\n",
      "/* 054 */           range_numElementsTodo_0 -= 1000L;\n",
      "/* 055 */         } else {\n",
      "/* 056 */           range_nextBatchTodo_0 = range_numElementsTodo_0;\n",
      "/* 057 */           range_numElementsTodo_0 = 0;\n",
      "/* 058 */           if (range_nextBatchTodo_0 == 0) break;\n",
      "/* 059 */         }\n",
      "/* 060 */         range_batchEnd_0 += range_nextBatchTodo_0 * 1L;\n",
      "/* 061 */       }\n",
      "/* 062 */\n",
      "/* 063 */       int range_localEnd_0 = (int)((range_batchEnd_0 - range_nextIndex_0) / 1L);\n",
      "/* 064 */       for (int range_localIdx_0 = 0; range_localIdx_0 < range_localEnd_0; range_localIdx_0++) {\n",
      "/* 065 */         long range_value_0 = ((long)range_localIdx_0 * 1L) + range_nextIndex_0;\n",
      "/* 066 */\n",
      "/* 067 */         do {\n",
      "/* 068 */           boolean filter_value_0 = false;\n",
      "/* 069 */           filter_value_0 = range_value_0 > 3L;\n",
      "/* 070 */           if (!filter_value_0) continue;\n",
      "/* 071 */\n",
      "/* 072 */           ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);\n",
      "/* 073 */\n",
      "/* 074 */           agg_doConsume_0(range_value_0);\n",
      "/* 075 */\n",
      "/* 076 */         } while(false);\n",
      "/* 077 */\n",
      "/* 078 */         // shouldStop check is eliminated\n",
      "/* 079 */       }\n",
      "/* 080 */       range_nextIndex_0 = range_batchEnd_0;\n",
      "/* 081 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(range_localEnd_0);\n",
      "/* 082 */       range_inputMetrics_0.incRecordsRead(range_localEnd_0);\n",
      "/* 083 */       range_taskContext_0.killTaskIfInterrupted();\n",
      "/* 084 */     }\n",
      "/* 085 */\n",
      "/* 086 */   }\n",
      "/* 087 */\n",
      "/* 088 */   private void initRange(int idx) {\n",
      "/* 089 */     java.math.BigInteger index = java.math.BigInteger.valueOf(idx);\n",
      "/* 090 */     java.math.BigInteger numSlice = java.math.BigInteger.valueOf(4L);\n",
      "/* 091 */     java.math.BigInteger numElement = java.math.BigInteger.valueOf(9L);\n",
      "/* 092 */     java.math.BigInteger step = java.math.BigInteger.valueOf(1L);\n",
      "/* 093 */     java.math.BigInteger start = java.math.BigInteger.valueOf(1L);\n",
      "/* 094 */     long partitionEnd;\n",
      "/* 095 */\n",
      "/* 096 */     java.math.BigInteger st = index.multiply(numElement).divide(numSlice).multiply(step).add(start);\n",
      "/* 097 */     if (st.compareTo(java.math.BigInteger.valueOf(Long.MAX_VALUE)) > 0) {\n",
      "/* 098 */       range_nextIndex_0 = Long.MAX_VALUE;\n",
      "/* 099 */     } else if (st.compareTo(java.math.BigInteger.valueOf(Long.MIN_VALUE)) < 0) {\n",
      "/* 100 */       range_nextIndex_0 = Long.MIN_VALUE;\n",
      "/* 101 */     } else {\n",
      "/* 102 */       range_nextIndex_0 = st.longValue();\n",
      "/* 103 */     }\n",
      "/* 104 */     range_batchEnd_0 = range_nextIndex_0;\n",
      "/* 105 */\n",
      "/* 106 */     java.math.BigInteger end = index.add(java.math.BigInteger.ONE).multiply(numElement).divide(numSlice)\n",
      "/* 107 */     .multiply(step).add(start);\n",
      "/* 108 */     if (end.compareTo(java.math.BigInteger.valueOf(Long.MAX_VALUE)) > 0) {\n",
      "/* 109 */       partitionEnd = Long.MAX_VALUE;\n",
      "/* 110 */     } else if (end.compareTo(java.math.BigInteger.valueOf(Long.MIN_VALUE)) < 0) {\n",
      "/* 111 */       partitionEnd = Long.MIN_VALUE;\n",
      "/* 112 */     } else {\n",
      "/* 113 */       partitionEnd = end.longValue();\n",
      "/* 114 */     }\n",
      "/* 115 */\n",
      "/* 116 */     java.math.BigInteger startToEnd = java.math.BigInteger.valueOf(partitionEnd).subtract(\n",
      "/* 117 */       java.math.BigInteger.valueOf(range_nextIndex_0));\n",
      "/* 118 */     range_numElementsTodo_0  = startToEnd.divide(step).longValue();\n",
      "/* 119 */     if (range_numElementsTodo_0 < 0) {\n",
      "/* 120 */       range_numElementsTodo_0 = 0;\n",
      "/* 121 */     } else if (startToEnd.remainder(step).compareTo(java.math.BigInteger.valueOf(0L)) != 0) {\n",
      "/* 122 */       range_numElementsTodo_0++;\n",
      "/* 123 */     }\n",
      "/* 124 */   }\n",
      "/* 125 */\n",
      "/* 126 */   private void agg_doConsume_0(long agg_expr_0_0) throws java.io.IOException {\n",
      "/* 127 */     // do aggregate\n",
      "/* 128 */     // common sub-expressions\n",
      "/* 129 */\n",
      "/* 130 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 131 */\n",
      "/* 132 */     agg_agg_isNull_3_0 = true;\n",
      "/* 133 */     long agg_value_3 = -1L;\n",
      "/* 134 */     do {\n",
      "/* 135 */       if (!agg_bufIsNull_0) {\n",
      "/* 136 */         agg_agg_isNull_3_0 = false;\n",
      "/* 137 */         agg_value_3 = agg_bufValue_0;\n",
      "/* 138 */         continue;\n",
      "/* 139 */       }\n",
      "/* 140 */\n",
      "/* 141 */       if (!false) {\n",
      "/* 142 */         agg_agg_isNull_3_0 = false;\n",
      "/* 143 */         agg_value_3 = 0L;\n",
      "/* 144 */         continue;\n",
      "/* 145 */       }\n",
      "/* 146 */\n",
      "/* 147 */     } while (false);\n",
      "/* 148 */\n",
      "/* 149 */     long agg_value_2 = -1L;\n",
      "/* 150 */\n",
      "/* 151 */     agg_value_2 = agg_value_3 + agg_expr_0_0;\n",
      "/* 152 */\n",
      "/* 153 */     agg_bufIsNull_0 = false;\n",
      "/* 154 */     agg_bufValue_0 = agg_value_2;\n",
      "/* 155 */\n",
      "/* 156 */   }\n",
      "/* 157 */\n",
      "/* 158 */   protected void processNext() throws java.io.IOException {\n",
      "/* 159 */     while (!agg_initAgg_0) {\n",
      "/* 160 */       agg_initAgg_0 = true;\n",
      "/* 161 */       long agg_beforeAgg_0 = System.nanoTime();\n",
      "/* 162 */       agg_doAggregateWithoutKey_0();\n",
      "/* 163 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);\n",
      "/* 164 */\n",
      "/* 165 */       // output the result\n",
      "/* 166 */\n",
      "/* 167 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* numOutputRows */).add(1);\n",
      "/* 168 */       range_mutableStateArray_0[3].reset();\n",
      "/* 169 */\n",
      "/* 170 */       range_mutableStateArray_0[3].zeroOutNullBytes();\n",
      "/* 171 */\n",
      "/* 172 */       if (agg_bufIsNull_0) {\n",
      "/* 173 */         range_mutableStateArray_0[3].setNullAt(0);\n",
      "/* 174 */       } else {\n",
      "/* 175 */         range_mutableStateArray_0[3].write(0, agg_bufValue_0);\n",
      "/* 176 */       }\n",
      "/* 177 */       append((range_mutableStateArray_0[3].getRow()));\n",
      "/* 178 */     }\n",
      "/* 179 */   }\n",
      "/* 180 */\n",
      "/* 181 */ }\n",
      "\n",
      "== Subtree 2 / 2 (maxMethodCodeSize:139; maxConstantPoolSize:137(0.21% used); numInnerClasses:0) ==\n",
      "*(2) HashAggregate(keys=[], functions=[sum(id#52L)], output=[sum(id)#56L])\n",
      "+- Exchange SinglePartition, true, [id=#141]\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_sum(id#52L)], output=[sum#59L])\n",
      "      +- *(1) Filter (id#52L > 3)\n",
      "         +- *(1) Range (1, 10, step=1, splits=4)\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=2\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean agg_initAgg_0;\n",
      "/* 010 */   private boolean agg_bufIsNull_0;\n",
      "/* 011 */   private long agg_bufValue_0;\n",
      "/* 012 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 013 */   private boolean agg_agg_isNull_3_0;\n",
      "/* 014 */   private boolean agg_agg_isNull_5_0;\n",
      "/* 015 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 016 */\n",
      "/* 017 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n",
      "/* 018 */     this.references = references;\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 022 */     partitionIndex = index;\n",
      "/* 023 */     this.inputs = inputs;\n",
      "/* 024 */\n",
      "/* 025 */     inputadapter_input_0 = inputs[0];\n",
      "/* 026 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 027 */\n",
      "/* 028 */   }\n",
      "/* 029 */\n",
      "/* 030 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {\n",
      "/* 031 */     // initialize aggregation buffer\n",
      "/* 032 */     agg_bufIsNull_0 = true;\n",
      "/* 033 */     agg_bufValue_0 = -1L;\n",
      "/* 034 */\n",
      "/* 035 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 036 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 037 */\n",
      "/* 038 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 039 */       long inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 040 */       -1L : (inputadapter_row_0.getLong(0));\n",
      "/* 041 */\n",
      "/* 042 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0);\n",
      "/* 043 */       // shouldStop check is eliminated\n",
      "/* 044 */     }\n",
      "/* 045 */\n",
      "/* 046 */   }\n",
      "/* 047 */\n",
      "/* 048 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, long agg_expr_0_0, boolean agg_exprIsNull_0_0) throws java.io.IOException {\n",
      "/* 049 */     // do aggregate\n",
      "/* 050 */     // common sub-expressions\n",
      "/* 051 */\n",
      "/* 052 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 053 */\n",
      "/* 054 */     agg_agg_isNull_3_0 = true;\n",
      "/* 055 */     long agg_value_3 = -1L;\n",
      "/* 056 */     do {\n",
      "/* 057 */       boolean agg_isNull_4 = true;\n",
      "/* 058 */       long agg_value_4 = -1L;\n",
      "/* 059 */       agg_agg_isNull_5_0 = true;\n",
      "/* 060 */       long agg_value_5 = -1L;\n",
      "/* 061 */       do {\n",
      "/* 062 */         if (!agg_bufIsNull_0) {\n",
      "/* 063 */           agg_agg_isNull_5_0 = false;\n",
      "/* 064 */           agg_value_5 = agg_bufValue_0;\n",
      "/* 065 */           continue;\n",
      "/* 066 */         }\n",
      "/* 067 */\n",
      "/* 068 */         if (!false) {\n",
      "/* 069 */           agg_agg_isNull_5_0 = false;\n",
      "/* 070 */           agg_value_5 = 0L;\n",
      "/* 071 */           continue;\n",
      "/* 072 */         }\n",
      "/* 073 */\n",
      "/* 074 */       } while (false);\n",
      "/* 075 */\n",
      "/* 076 */       if (!agg_exprIsNull_0_0) {\n",
      "/* 077 */         agg_isNull_4 = false; // resultCode could change nullability.\n",
      "/* 078 */\n",
      "/* 079 */         agg_value_4 = agg_value_5 + agg_expr_0_0;\n",
      "/* 080 */\n",
      "/* 081 */       }\n",
      "/* 082 */       if (!agg_isNull_4) {\n",
      "/* 083 */         agg_agg_isNull_3_0 = false;\n",
      "/* 084 */         agg_value_3 = agg_value_4;\n",
      "/* 085 */         continue;\n",
      "/* 086 */       }\n",
      "/* 087 */\n",
      "/* 088 */       if (!agg_bufIsNull_0) {\n",
      "/* 089 */         agg_agg_isNull_3_0 = false;\n",
      "/* 090 */         agg_value_3 = agg_bufValue_0;\n",
      "/* 091 */         continue;\n",
      "/* 092 */       }\n",
      "/* 093 */\n",
      "/* 094 */     } while (false);\n",
      "/* 095 */\n",
      "/* 096 */     agg_bufIsNull_0 = agg_agg_isNull_3_0;\n",
      "/* 097 */     agg_bufValue_0 = agg_value_3;\n",
      "/* 098 */\n",
      "/* 099 */   }\n",
      "/* 100 */\n",
      "/* 101 */   protected void processNext() throws java.io.IOException {\n",
      "/* 102 */     while (!agg_initAgg_0) {\n",
      "/* 103 */       agg_initAgg_0 = true;\n",
      "/* 104 */       long agg_beforeAgg_0 = System.nanoTime();\n",
      "/* 105 */       agg_doAggregateWithoutKey_0();\n",
      "/* 106 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);\n",
      "/* 107 */\n",
      "/* 108 */       // output the result\n",
      "/* 109 */\n",
      "/* 110 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 111 */       agg_mutableStateArray_0[0].reset();\n",
      "/* 112 */\n",
      "/* 113 */       agg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 114 */\n",
      "/* 115 */       if (agg_bufIsNull_0) {\n",
      "/* 116 */         agg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 117 */       } else {\n",
      "/* 118 */         agg_mutableStateArray_0[0].write(0, agg_bufValue_0);\n",
      "/* 119 */       }\n",
      "/* 120 */       append((agg_mutableStateArray_0[0].getRow()));\n",
      "/* 121 */     }\n",
      "/* 122 */   }\n",
      "/* 123 */\n",
      "/* 124 */ }\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "q.queryExecution.debug.codegen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Explain Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[sum(id#52L)])\n",
      "+- Exchange SinglePartition, true, [id=#141]\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_sum(id#52L)])\n",
      "      +- *(1) Filter (id#52L > 3)\n",
      "         +- *(1) Range (1, 10, step=1, splits=4)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q.explain(\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('sum('id), Some(org.apache.spark.sql.Column$$Lambda$2986/0x0000000841215040@2d4cb313))]\n",
      "+- Filter (id#52L > cast(3 as bigint))\n",
      "   +- Range (1, 10, step=1, splits=Some(4))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "sum(id): bigint\n",
      "Aggregate [sum(id#52L) AS sum(id)#56L]\n",
      "+- Filter (id#52L > cast(3 as bigint))\n",
      "   +- Range (1, 10, step=1, splits=Some(4))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [sum(id#52L) AS sum(id)#56L]\n",
      "+- Filter (id#52L > 3)\n",
      "   +- Range (1, 10, step=1, splits=Some(4))\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[sum(id#52L)], output=[sum(id)#56L])\n",
      "+- Exchange SinglePartition, true, [id=#141]\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_sum(id#52L)], output=[sum#59L])\n",
      "      +- *(1) Filter (id#52L > 3)\n",
      "         +- *(1) Range (1, 10, step=1, splits=4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q.explain(\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 WholeStageCodegen subtrees.\n",
      "== Subtree 1 / 2 (maxMethodCodeSize:282; maxConstantPoolSize:208(0.32% used); numInnerClasses:0) ==\n",
      "*(1) HashAggregate(keys=[], functions=[partial_sum(id#52L)], output=[sum#59L])\n",
      "+- *(1) Filter (id#52L > 3)\n",
      "   +- *(1) Range (1, 10, step=1, splits=4)\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean agg_initAgg_0;\n",
      "/* 010 */   private boolean agg_bufIsNull_0;\n",
      "/* 011 */   private long agg_bufValue_0;\n",
      "/* 012 */   private boolean range_initRange_0;\n",
      "/* 013 */   private long range_nextIndex_0;\n",
      "/* 014 */   private TaskContext range_taskContext_0;\n",
      "/* 015 */   private InputMetrics range_inputMetrics_0;\n",
      "/* 016 */   private long range_batchEnd_0;\n",
      "/* 017 */   private long range_numElementsTodo_0;\n",
      "/* 018 */   private boolean agg_agg_isNull_3_0;\n",
      "/* 019 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] range_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[4];\n",
      "/* 020 */\n",
      "/* 021 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 022 */     this.references = references;\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 026 */     partitionIndex = index;\n",
      "/* 027 */     this.inputs = inputs;\n",
      "/* 028 */\n",
      "/* 029 */     range_taskContext_0 = TaskContext.get();\n",
      "/* 030 */     range_inputMetrics_0 = range_taskContext_0.taskMetrics().inputMetrics();\n",
      "/* 031 */     range_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 032 */     range_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 033 */     range_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 034 */     range_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 035 */\n",
      "/* 036 */   }\n",
      "/* 037 */\n",
      "/* 038 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {\n",
      "/* 039 */     // initialize aggregation buffer\n",
      "/* 040 */     agg_bufIsNull_0 = true;\n",
      "/* 041 */     agg_bufValue_0 = -1L;\n",
      "/* 042 */\n",
      "/* 043 */     // initialize Range\n",
      "/* 044 */     if (!range_initRange_0) {\n",
      "/* 045 */       range_initRange_0 = true;\n",
      "/* 046 */       initRange(partitionIndex);\n",
      "/* 047 */     }\n",
      "/* 048 */\n",
      "/* 049 */     while (true) {\n",
      "/* 050 */       if (range_nextIndex_0 == range_batchEnd_0) {\n",
      "/* 051 */         long range_nextBatchTodo_0;\n",
      "/* 052 */         if (range_numElementsTodo_0 > 1000L) {\n",
      "/* 053 */           range_nextBatchTodo_0 = 1000L;\n",
      "/* 054 */           range_numElementsTodo_0 -= 1000L;\n",
      "/* 055 */         } else {\n",
      "/* 056 */           range_nextBatchTodo_0 = range_numElementsTodo_0;\n",
      "/* 057 */           range_numElementsTodo_0 = 0;\n",
      "/* 058 */           if (range_nextBatchTodo_0 == 0) break;\n",
      "/* 059 */         }\n",
      "/* 060 */         range_batchEnd_0 += range_nextBatchTodo_0 * 1L;\n",
      "/* 061 */       }\n",
      "/* 062 */\n",
      "/* 063 */       int range_localEnd_0 = (int)((range_batchEnd_0 - range_nextIndex_0) / 1L);\n",
      "/* 064 */       for (int range_localIdx_0 = 0; range_localIdx_0 < range_localEnd_0; range_localIdx_0++) {\n",
      "/* 065 */         long range_value_0 = ((long)range_localIdx_0 * 1L) + range_nextIndex_0;\n",
      "/* 066 */\n",
      "/* 067 */         do {\n",
      "/* 068 */           boolean filter_value_0 = false;\n",
      "/* 069 */           filter_value_0 = range_value_0 > 3L;\n",
      "/* 070 */           if (!filter_value_0) continue;\n",
      "/* 071 */\n",
      "/* 072 */           ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);\n",
      "/* 073 */\n",
      "/* 074 */           agg_doConsume_0(range_value_0);\n",
      "/* 075 */\n",
      "/* 076 */         } while(false);\n",
      "/* 077 */\n",
      "/* 078 */         // shouldStop check is eliminated\n",
      "/* 079 */       }\n",
      "/* 080 */       range_nextIndex_0 = range_batchEnd_0;\n",
      "/* 081 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(range_localEnd_0);\n",
      "/* 082 */       range_inputMetrics_0.incRecordsRead(range_localEnd_0);\n",
      "/* 083 */       range_taskContext_0.killTaskIfInterrupted();\n",
      "/* 084 */     }\n",
      "/* 085 */\n",
      "/* 086 */   }\n",
      "/* 087 */\n",
      "/* 088 */   private void initRange(int idx) {\n",
      "/* 089 */     java.math.BigInteger index = java.math.BigInteger.valueOf(idx);\n",
      "/* 090 */     java.math.BigInteger numSlice = java.math.BigInteger.valueOf(4L);\n",
      "/* 091 */     java.math.BigInteger numElement = java.math.BigInteger.valueOf(9L);\n",
      "/* 092 */     java.math.BigInteger step = java.math.BigInteger.valueOf(1L);\n",
      "/* 093 */     java.math.BigInteger start = java.math.BigInteger.valueOf(1L);\n",
      "/* 094 */     long partitionEnd;\n",
      "/* 095 */\n",
      "/* 096 */     java.math.BigInteger st = index.multiply(numElement).divide(numSlice).multiply(step).add(start);\n",
      "/* 097 */     if (st.compareTo(java.math.BigInteger.valueOf(Long.MAX_VALUE)) > 0) {\n",
      "/* 098 */       range_nextIndex_0 = Long.MAX_VALUE;\n",
      "/* 099 */     } else if (st.compareTo(java.math.BigInteger.valueOf(Long.MIN_VALUE)) < 0) {\n",
      "/* 100 */       range_nextIndex_0 = Long.MIN_VALUE;\n",
      "/* 101 */     } else {\n",
      "/* 102 */       range_nextIndex_0 = st.longValue();\n",
      "/* 103 */     }\n",
      "/* 104 */     range_batchEnd_0 = range_nextIndex_0;\n",
      "/* 105 */\n",
      "/* 106 */     java.math.BigInteger end = index.add(java.math.BigInteger.ONE).multiply(numElement).divide(numSlice)\n",
      "/* 107 */     .multiply(step).add(start);\n",
      "/* 108 */     if (end.compareTo(java.math.BigInteger.valueOf(Long.MAX_VALUE)) > 0) {\n",
      "/* 109 */       partitionEnd = Long.MAX_VALUE;\n",
      "/* 110 */     } else if (end.compareTo(java.math.BigInteger.valueOf(Long.MIN_VALUE)) < 0) {\n",
      "/* 111 */       partitionEnd = Long.MIN_VALUE;\n",
      "/* 112 */     } else {\n",
      "/* 113 */       partitionEnd = end.longValue();\n",
      "/* 114 */     }\n",
      "/* 115 */\n",
      "/* 116 */     java.math.BigInteger startToEnd = java.math.BigInteger.valueOf(partitionEnd).subtract(\n",
      "/* 117 */       java.math.BigInteger.valueOf(range_nextIndex_0));\n",
      "/* 118 */     range_numElementsTodo_0  = startToEnd.divide(step).longValue();\n",
      "/* 119 */     if (range_numElementsTodo_0 < 0) {\n",
      "/* 120 */       range_numElementsTodo_0 = 0;\n",
      "/* 121 */     } else if (startToEnd.remainder(step).compareTo(java.math.BigInteger.valueOf(0L)) != 0) {\n",
      "/* 122 */       range_numElementsTodo_0++;\n",
      "/* 123 */     }\n",
      "/* 124 */   }\n",
      "/* 125 */\n",
      "/* 126 */   private void agg_doConsume_0(long agg_expr_0_0) throws java.io.IOException {\n",
      "/* 127 */     // do aggregate\n",
      "/* 128 */     // common sub-expressions\n",
      "/* 129 */\n",
      "/* 130 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 131 */\n",
      "/* 132 */     agg_agg_isNull_3_0 = true;\n",
      "/* 133 */     long agg_value_3 = -1L;\n",
      "/* 134 */     do {\n",
      "/* 135 */       if (!agg_bufIsNull_0) {\n",
      "/* 136 */         agg_agg_isNull_3_0 = false;\n",
      "/* 137 */         agg_value_3 = agg_bufValue_0;\n",
      "/* 138 */         continue;\n",
      "/* 139 */       }\n",
      "/* 140 */\n",
      "/* 141 */       if (!false) {\n",
      "/* 142 */         agg_agg_isNull_3_0 = false;\n",
      "/* 143 */         agg_value_3 = 0L;\n",
      "/* 144 */         continue;\n",
      "/* 145 */       }\n",
      "/* 146 */\n",
      "/* 147 */     } while (false);\n",
      "/* 148 */\n",
      "/* 149 */     long agg_value_2 = -1L;\n",
      "/* 150 */\n",
      "/* 151 */     agg_value_2 = agg_value_3 + agg_expr_0_0;\n",
      "/* 152 */\n",
      "/* 153 */     agg_bufIsNull_0 = false;\n",
      "/* 154 */     agg_bufValue_0 = agg_value_2;\n",
      "/* 155 */\n",
      "/* 156 */   }\n",
      "/* 157 */\n",
      "/* 158 */   protected void processNext() throws java.io.IOException {\n",
      "/* 159 */     while (!agg_initAgg_0) {\n",
      "/* 160 */       agg_initAgg_0 = true;\n",
      "/* 161 */       long agg_beforeAgg_0 = System.nanoTime();\n",
      "/* 162 */       agg_doAggregateWithoutKey_0();\n",
      "/* 163 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);\n",
      "/* 164 */\n",
      "/* 165 */       // output the result\n",
      "/* 166 */\n",
      "/* 167 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* numOutputRows */).add(1);\n",
      "/* 168 */       range_mutableStateArray_0[3].reset();\n",
      "/* 169 */\n",
      "/* 170 */       range_mutableStateArray_0[3].zeroOutNullBytes();\n",
      "/* 171 */\n",
      "/* 172 */       if (agg_bufIsNull_0) {\n",
      "/* 173 */         range_mutableStateArray_0[3].setNullAt(0);\n",
      "/* 174 */       } else {\n",
      "/* 175 */         range_mutableStateArray_0[3].write(0, agg_bufValue_0);\n",
      "/* 176 */       }\n",
      "/* 177 */       append((range_mutableStateArray_0[3].getRow()));\n",
      "/* 178 */     }\n",
      "/* 179 */   }\n",
      "/* 180 */\n",
      "/* 181 */ }\n",
      "\n",
      "== Subtree 2 / 2 (maxMethodCodeSize:139; maxConstantPoolSize:137(0.21% used); numInnerClasses:0) ==\n",
      "*(2) HashAggregate(keys=[], functions=[sum(id#52L)], output=[sum(id)#56L])\n",
      "+- Exchange SinglePartition, true, [id=#141]\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_sum(id#52L)], output=[sum#59L])\n",
      "      +- *(1) Filter (id#52L > 3)\n",
      "         +- *(1) Range (1, 10, step=1, splits=4)\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=2\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean agg_initAgg_0;\n",
      "/* 010 */   private boolean agg_bufIsNull_0;\n",
      "/* 011 */   private long agg_bufValue_0;\n",
      "/* 012 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 013 */   private boolean agg_agg_isNull_3_0;\n",
      "/* 014 */   private boolean agg_agg_isNull_5_0;\n",
      "/* 015 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 016 */\n",
      "/* 017 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n",
      "/* 018 */     this.references = references;\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 022 */     partitionIndex = index;\n",
      "/* 023 */     this.inputs = inputs;\n",
      "/* 024 */\n",
      "/* 025 */     inputadapter_input_0 = inputs[0];\n",
      "/* 026 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 027 */\n",
      "/* 028 */   }\n",
      "/* 029 */\n",
      "/* 030 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {\n",
      "/* 031 */     // initialize aggregation buffer\n",
      "/* 032 */     agg_bufIsNull_0 = true;\n",
      "/* 033 */     agg_bufValue_0 = -1L;\n",
      "/* 034 */\n",
      "/* 035 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 036 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 037 */\n",
      "/* 038 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 039 */       long inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 040 */       -1L : (inputadapter_row_0.getLong(0));\n",
      "/* 041 */\n",
      "/* 042 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0);\n",
      "/* 043 */       // shouldStop check is eliminated\n",
      "/* 044 */     }\n",
      "/* 045 */\n",
      "/* 046 */   }\n",
      "/* 047 */\n",
      "/* 048 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, long agg_expr_0_0, boolean agg_exprIsNull_0_0) throws java.io.IOException {\n",
      "/* 049 */     // do aggregate\n",
      "/* 050 */     // common sub-expressions\n",
      "/* 051 */\n",
      "/* 052 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 053 */\n",
      "/* 054 */     agg_agg_isNull_3_0 = true;\n",
      "/* 055 */     long agg_value_3 = -1L;\n",
      "/* 056 */     do {\n",
      "/* 057 */       boolean agg_isNull_4 = true;\n",
      "/* 058 */       long agg_value_4 = -1L;\n",
      "/* 059 */       agg_agg_isNull_5_0 = true;\n",
      "/* 060 */       long agg_value_5 = -1L;\n",
      "/* 061 */       do {\n",
      "/* 062 */         if (!agg_bufIsNull_0) {\n",
      "/* 063 */           agg_agg_isNull_5_0 = false;\n",
      "/* 064 */           agg_value_5 = agg_bufValue_0;\n",
      "/* 065 */           continue;\n",
      "/* 066 */         }\n",
      "/* 067 */\n",
      "/* 068 */         if (!false) {\n",
      "/* 069 */           agg_agg_isNull_5_0 = false;\n",
      "/* 070 */           agg_value_5 = 0L;\n",
      "/* 071 */           continue;\n",
      "/* 072 */         }\n",
      "/* 073 */\n",
      "/* 074 */       } while (false);\n",
      "/* 075 */\n",
      "/* 076 */       if (!agg_exprIsNull_0_0) {\n",
      "/* 077 */         agg_isNull_4 = false; // resultCode could change nullability.\n",
      "/* 078 */\n",
      "/* 079 */         agg_value_4 = agg_value_5 + agg_expr_0_0;\n",
      "/* 080 */\n",
      "/* 081 */       }\n",
      "/* 082 */       if (!agg_isNull_4) {\n",
      "/* 083 */         agg_agg_isNull_3_0 = false;\n",
      "/* 084 */         agg_value_3 = agg_value_4;\n",
      "/* 085 */         continue;\n",
      "/* 086 */       }\n",
      "/* 087 */\n",
      "/* 088 */       if (!agg_bufIsNull_0) {\n",
      "/* 089 */         agg_agg_isNull_3_0 = false;\n",
      "/* 090 */         agg_value_3 = agg_bufValue_0;\n",
      "/* 091 */         continue;\n",
      "/* 092 */       }\n",
      "/* 093 */\n",
      "/* 094 */     } while (false);\n",
      "/* 095 */\n",
      "/* 096 */     agg_bufIsNull_0 = agg_agg_isNull_3_0;\n",
      "/* 097 */     agg_bufValue_0 = agg_value_3;\n",
      "/* 098 */\n",
      "/* 099 */   }\n",
      "/* 100 */\n",
      "/* 101 */   protected void processNext() throws java.io.IOException {\n",
      "/* 102 */     while (!agg_initAgg_0) {\n",
      "/* 103 */       agg_initAgg_0 = true;\n",
      "/* 104 */       long agg_beforeAgg_0 = System.nanoTime();\n",
      "/* 105 */       agg_doAggregateWithoutKey_0();\n",
      "/* 106 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);\n",
      "/* 107 */\n",
      "/* 108 */       // output the result\n",
      "/* 109 */\n",
      "/* 110 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 111 */       agg_mutableStateArray_0[0].reset();\n",
      "/* 112 */\n",
      "/* 113 */       agg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 114 */\n",
      "/* 115 */       if (agg_bufIsNull_0) {\n",
      "/* 116 */         agg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 117 */       } else {\n",
      "/* 118 */         agg_mutableStateArray_0[0].write(0, agg_bufValue_0);\n",
      "/* 119 */       }\n",
      "/* 120 */       append((agg_mutableStateArray_0[0].getRow()));\n",
      "/* 121 */     }\n",
      "/* 122 */   }\n",
      "/* 123 */\n",
      "/* 124 */ }\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q.explain(\"codegen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "Aggregate [sum(id#52L) AS sum(id)#56L], Statistics(sizeInBytes=16.0 B, rowCount=1)\n",
      "+- Filter (id#52L > 3)\n",
      "   +- Range (1, 10, step=1, splits=Some(4))\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[sum(id#52L)], output=[sum(id)#56L])\n",
      "+- Exchange SinglePartition, true, [id=#141]\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_sum(id#52L)], output=[sum#59L])\n",
      "      +- *(1) Filter (id#52L > 3)\n",
      "         +- *(1) Range (1, 10, step=1, splits=4)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q.explain(\"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* HashAggregate (5)\n",
      "+- Exchange (4)\n",
      "   +- * HashAggregate (3)\n",
      "      +- * Filter (2)\n",
      "         +- * Range (1)\n",
      "\n",
      "\n",
      "(1) Range [codegen id : 1]\n",
      "Output [1]: [id#52L]\n",
      "Arguments: Range (1, 10, step=1, splits=Some(4))\n",
      "\n",
      "(2) Filter [codegen id : 1]\n",
      "Input [1]: [id#52L]\n",
      "Condition : (id#52L > 3)\n",
      "\n",
      "(3) HashAggregate [codegen id : 1]\n",
      "Input [1]: [id#52L]\n",
      "Keys: []\n",
      "Functions [1]: [partial_sum(id#52L)]\n",
      "Aggregate Attributes [1]: [sum#58L]\n",
      "Results [1]: [sum#59L]\n",
      "\n",
      "(4) Exchange\n",
      "Input [1]: [sum#59L]\n",
      "Arguments: SinglePartition, true, [id=#141]\n",
      "\n",
      "(5) HashAggregate [codegen id : 2]\n",
      "Input [1]: [sum#59L]\n",
      "Keys: []\n",
      "Functions [1]: [sum(id#52L)]\n",
      "Aggregate Attributes [1]: [sum(id#52L)#55L]\n",
      "Results [1]: [sum(id#52L)#55L AS sum(id)#56L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q.explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
